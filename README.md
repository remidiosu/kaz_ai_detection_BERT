# KAZ BERT AI DETECTION 

This is a project for fine-tuning BERT based models for AI detection in kazakh language. 
The base model used was kz-transformers/kaz-roberta-conversational from Hugging Face. 
Dataset includes 2,000 human texts from kz-transformers/multidomain-kazakh-dataset on Hugging Face and 2000 GPT paraphrases of the same texts 

The project also contains code for: 
    1. downloading the data from "kz-transformers/multidomain-kazakh-dataset"
    2. splitting, cleaning, then paraphrasing the given data using GPT based models from OPENAI
    3. using this data to train the model on the pretrained kz-transformers/kaz-roberta-conversational

## Results of the fine tuned model on test set

1. kaz-roberta-train
    'test_loss': 0.36990347504615784, 
    'test_accuracy': 0.86375, 
    'test_f1': 0.8685162846803377, 
    'test_precision': 0.8391608391608392, 
    'test_recall': 0.9

2. kazBERT-train



## Dir structure
- main.py: 
    - uncomment the functions you need and run

- test.py:
    - run to perform test of the trained model on test dataset, get eval metrics and graphs 

- kaz-roberta-train/
    - checkpoints for fine-tuned KAZ_BERT_AI_DETECTION 
- configs/
    - data.yaml: config file for dowloading the data from "kz-transformers/multidomain-kazakh-dataset"
    - generate.yaml: config for using OPENAI API to generate paraphrases
        there you can define what models, temperatures and prompts and their corresponding weights to use for paraphrases
        (prompts dont have weights and are picked randomly)
    - train.yaml: config file that contains train arguments, pretrained name and output dir

- data/
    - corpus/ contains the splits for test/train/validation for training the model
        - {split}_human.csv: cleaned human texts (generated by get_data/split.py)
        - {split}_ai_partials.csv: we put gpt paraphrases into this file, along with temperature, model and prompt
        - {split}.csv: contains the final dataset with mixed texts from humans and AI
    - interim/
        - contains uncleaned kazakh texts downloaded from kz-transformers divided by categories

- get_data/ 
    - get_data.py: download the data from "kz-transformers/multidomain-kazakh-dataset" and save it in data/interim
    - split.py: read all files from data/interim, shuffle, split, clean and save it in data/{split}/{split}_human.csv
    - generate_ai.py: read {split}_human.csv for each split, then call OPENAI to paraphrase
        saves output to {split}_ai_partials.csv while paraphrasing
        saves the final file into {split}.csv in the end of each split
        saves the logs of each paraphrase call into logs/
    - chunking.py: 
        Split `text` into manageable chunks, paraphrase each via the OpenAI client,
        and reassemble the outputs into one string.
    - utils.py: contains just one util function to read config files
    - clean.py: contains the cleaning rules and functions for the dataset
        If the data has more than 20% of non cyrillic letters -> rewrite those letters into cyrillic equivalent
        If less, then delete the letters in non cyrillic
        (Keep in mind that at first, when I was cleaning the human texts, I deleted all the non cyrillic letters 
        Then GPT started sometimes returning paraphrases almost entirely in latin (kazakh can be written in latin too) so I had
        to change the cleaning rules after all paraphrases were done) 

- logs/
    - contain logs from get_data/generate_ai.py

- reports/
    - contains test reports generated by text.py 

- src/ 
    - dataset.py: defines torch dataset
    - eval_metrics.py: defines the metrics 
    - model.py: defines the torch model, right now the model handles tokenization inside its forward() 
        which slows the train time A LOT. It was initially done to get vectors representing the whole
        document not chunks, however I later realised how inefficient it is. Consider rewriting if you want more fast train time
    - preprocess.py: reads the data/corpus files, clean and return them as pd df
    - train.py: reads train.yaml and train the model

